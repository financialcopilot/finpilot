# Stage 1: backend/services/evaluation_service.py

import os
import json
from dotenv import load_dotenv
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI
from google.api_core.exceptions import ResourceExhausted

# Load environment variables
load_dotenv()

# --- API Keys Loader ---
API_KEYS = os.getenv("GOOGLE_API_KEYS", "").split(',')
if not all(API_KEYS) or API_KEYS == ['']:
    raise ValueError("GOOGLE_API_KEYS environment variable not set or is empty. Please check your .env file.")


# --- NEW: Resilient LLM Invoker with Key Cycling ---
async def invoke_llm_with_retry(prompt_template, input_data, temperature: float = 0.7):
    """
    Tries to invoke a LangChain chain with a list of API keys.
    If a key is rate-limited (ResourceExhausted), it automatically retries with the next key.
    """
    for i, key in enumerate(API_KEYS):
        try:
            # Initialize a new LLM instance with the current key
            llm = ChatGoogleGenerativeAI(
                model="gemini-1.5-flash-latest",
                temperature=temperature,
                google_api_key=key
            )

            # Build the full chain using the prompt and the LLM
            chain = prompt_template | llm | StrOutputParser()

            print(f"--- Attempting API call with Key #{i + 1} ---")
            response = await chain.ainvoke(input_data)
            print(f"--- Key #{i + 1} succeeded. ---")
            return response  # Success, return response

        except ResourceExhausted:
            print(f"Warning: API Key #{i + 1} is rate-limited or exhausted. Trying next key...")
            if i == len(API_KEYS) - 1:  # If this was the last key
                print("Error: All API keys are exhausted.")
                raise
        except Exception as e:
            print(f"An unexpected error occurred with Key #{i + 1}: {e}")
            if i == len(API_KEYS) - 1:
                raise

    raise Exception("All API keys failed to generate a response.")


# --- 1. Golden Principles: Programmatic, Objective Checks ---
def check_golden_principles(user_profile: dict, generated_plan: dict) -> dict:
    results = {}

    # Principle 1: Debt Management
    if user_profile.get('liabilities', {}).get('high_interest_debt', 0) > 1000:
        sentinel_rec_lower = ' '.join(generated_plan.get('sentinel_plan', {}).get('recommendations', [])).lower()
        voyager_rec_lower = ' '.join(generated_plan.get('voyager_plan', {}).get('recommendations', [])).lower()
        results['debt_priority_check'] = 'debt' in sentinel_rec_lower and 'debt' in voyager_rec_lower
    else:
        results['debt_priority_check'] = True

    # Principle 2: Risk Profile Alignment
    try:
        risk_score = sum(user_profile.get('risk_profile_answers', []))

        if risk_score > 4:  # Aggressive
            voyager_equity_str = generated_plan.get('voyager_plan', {}).get('asset_allocation', {}).get('equities', '0%')
            voyager_equity_pct = float(voyager_equity_str.replace('%', ''))
            results['risk_profile_alignment_check'] = voyager_equity_pct >= 60.0
        elif risk_score <= 3:  # Conservative
            sentinel_equity_str = generated_plan.get('sentinel_plan', {}).get('asset_allocation', {}).get('equities', '0%')
            sentinel_equity_pct = float(sentinel_equity_str.replace('%', ''))
            results['risk_profile_alignment_check'] = sentinel_equity_pct <= 40.0
        else:  # Balanced
            results['risk_profile_alignment_check'] = True
    except (ValueError, TypeError):
        results['risk_profile_alignment_check'] = False

    return results


# --- 2. LLM-as-Judge: The Evaluator Agent ---
evaluator_template = """
You are a Quality Assurance AI Agent, an impartial judge responsible for evaluating the quality of financial plans generated by another AI system.
Your analysis must be critical, objective, and strictly based on the provided data.

Your Task:
1. Verify Golden Principles: First, check if the Generated Plan adheres to the Golden Principles based on the User Profile.
2. Score on Quality Criteria: Grade the plan's quality on three subjective criteria, providing a score from 1-10 and a brief justification for each.
3. Final Verdict: Provide a final overall score and a concluding summary.

CRITICAL INSTRUCTION: Your entire output MUST be a single, valid JSON object with no other text, comments, or explanations before or after it.

CONTEXT - USER'S ORIGINAL PROFILE:
{user_profile}

CONTEXT - THE GENERATED PLAN TO EVALUATE:
{generated_plan}

EVALUATION RUBRIC & JSON OUTPUT STRUCTURE:
{{
  "golden_principle_reasoning": {{
    "debt_priority_check": "1 sentence reasoning",
    "risk_profile_alignment_check": "1 sentence reasoning"
  }},
  "quality_scores": {{
    "personalization": {{
      "score": "1-10",
      "reasoning": "Short justification"
    }},
    "actionability": {{
      "score": "1-10",
      "reasoning": "Short justification"
    }},
    "clarity_and_tone": {{
      "score": "1-10",
      "reasoning": "Short justification"
    }}
  }},
  "final_verdict": {{
    "overall_score": "Average of the 3 quality scores",
    "summary": "1 sentence summary"
  }}
}}
"""

evaluator_prompt = PromptTemplate(
    template=evaluator_template,
    input_variables=["user_profile", "generated_plan"]
)


# --- 3. The Main Orchestrator Function ---
async def evaluate_plan(user_profile: dict, generated_plan: dict) -> dict:
    print("--- Starting Hybrid Evaluation Process ---")

    # Layer 1: Objective checks
    golden_principle_results = check_golden_principles(user_profile, generated_plan)
    print(f"Golden Principles Check Results: {golden_principle_results}")

    # Layer 2: Subjective evaluation with key-cycling
    evaluator_input = {
        "user_profile": json.dumps(user_profile),
        "generated_plan": json.dumps(generated_plan)
    }

    evaluation_str = await invoke_llm_with_retry(
        prompt_template=evaluator_prompt,
        input_data=evaluator_input,
        temperature=0.1  # deterministic for evaluator
    )

    # Parse JSON from LLM
    start_index = evaluation_str.find('{')
    end_index = evaluation_str.rfind('}') + 1
    if start_index == -1 or end_index == 0:
        raise ValueError("Evaluator agent returned invalid data (no JSON object found).")
    json_str = evaluation_str[start_index:end_index]
    ai_evaluation_results = json.loads(json_str)

    print("AI Judge Evaluation Complete.")

    # Final combined report
    return {
        "golden_principle_checks": golden_principle_results,
        "ai_evaluation": ai_evaluation_results
    }